# -*- coding: utf-8 -*-
"""
Created on Mon Jan 30 08:45:52 2023

@author: Amara Dalila
"""
"""
Created on Sat Jan 28 06:19:46 2023

@author: Amara Dalila
"""
from google.colab import drive
drive.mount('/content/drive')
 
# Data manipulation
import pandas as pd # for data manipulation
 
# Sklearn
import sklearn # for model evaluation
from sklearn.preprocessing import MinMaxScaler # For rescaling metrics to fit into 0 to 1 range
from sklearn.model_selection import train_test_split # for splitting the data into train and test samples

# Visualization
import matplotlib 
import matplotlib.pyplot as plt # for plotting model loss
import graphviz # for showing model diagram

df = pd.read_csv('/content/drive/My Drive/Data/xerces-1.3.csv')
#df=pd.from_csv('Data/churn.csv', index_col=None);
#df.columns = df.columns.str.strip()
 

#Step 3: Separating the dependent 'bugs' and independent variables 'metrics'




# Separating the normal and fraudulent transactions

target=df[df.columns[-1]]


fraud = target!= 0
normal = target== 0

 
# Separating the dependent and independent variables
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler 
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
from keras.layers import Input, Dense
from keras.models import Model, Sequential
from keras import regularizers
import numpy as np

df[df.columns[-1]]=df[df.columns[-1]].apply(lambda x: 1 if x!=0 else 0)
 


y=df[df.columns[-1]]
X = df.drop(df.columns[-1], axis = 1)
X = df.drop(df.columns[0:3], axis = 1)


# Scaling the data to make it suitable for the auto-encoder
X_scaled = MinMaxScaler().fit_transform(X)
X_normal_scaled = X_scaled[y == 0]
X_fraud_scaled = X_scaled[y != 0]

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_scaled_resampled, y_resampled = smote.fit_resample(X_scaled, y)


#Step 7: Building the Auto-encoder neural network

# Building the Input Layer
input_layer = Input(shape =(X.shape[1], ))
  
# Building the Encoder network
encoded = Dense(100, activation ='sigmoid',
                activity_regularizer = regularizers.l1(10e-5))(input_layer)
encoded = Dense(50, activation ='sigmoid',
                activity_regularizer = regularizers.l1(10e-5))(encoded)
encoded = Dense(25, activation ='sigmoid',
                activity_regularizer = regularizers.l1(10e-5))(encoded)
encoded = Dense(12, activation ='sigmoid',
                activity_regularizer = regularizers.l1(10e-5))(encoded)
encoded = Dense(6, activation ='sigmoid')(encoded)
  
# Building the Decoder network
decoded = Dense(12, activation ='sigmoid')(encoded)
decoded = Dense(25, activation ='sigmoid')(decoded)
decoded = Dense(50, activation ='sigmoid')(decoded)
decoded = Dense(100, activation ='sigmoid')(decoded)
  
# Building the Output Layer
output_layer = Dense(X.shape[1], activation ='sigmoid')(decoded)


#Step 8: Defining and Training the Auto-encoder

# Defining the parameters of the Auto-encoder network
autoencoder = Model(input_layer, output_layer)
autoencoder.compile(optimizer='adam', loss ="mse", metrics=['accuracy'])
 
 

import time
start = time.time()
SAE_history=autoencoder.fit(X_scaled_resampled, X_scaled_resampled, 
                batch_size = 16, epochs = 100, 
                shuffle = True, validation_split = 0.20)
stop = time.time()
print(f"Training time: {stop - start}s")


#---------- Plot a loss chart ----------#
fig, ax = plt.subplots(figsize=(16,9), dpi=300)
plt.title(label='SAE Model Loss by Epoch', loc='center')
ax.plot(SAE_history.history['loss'], label='Training Data', color='blue')
ax.plot(SAE_history.history['val_loss'], label='Test Data', color='orange')
ax.set(xlabel='Epoch', ylabel='Loss')
plt.legend()
plt.show()

# Plot Model Loss and Model accuracy
    # list all data in history
plt.figure(1)

 

# summarize history for accuracy

plt.subplot(211)
plt.plot(SAE_history.history['accuracy'])
plt.plot(SAE_history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc='lower right')


# summarize history for loss

plt.subplot(212)
plt.plot(SAE_history.history['loss'])
plt.plot(SAE_history.history['val_loss'])
plt.title('Model Loss "xerces-1.3" ')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc='upper right')

plt.tight_layout()

plt.show()
 


#Step 9: Retaining the encoder part of the Auto-encoder to encode data

hidden_representation = Sequential()
hidden_representation.add(autoencoder.layers[0])
hidden_representation.add(autoencoder.layers[1])
hidden_representation.add(autoencoder.layers[2])
hidden_representation.add(autoencoder.layers[3])
hidden_representation.add(autoencoder.layers[4])


#Step 10: Encoding the data and visualizing the encoded data

# Separating the points encoded by the Auto-encoder as normal and fraud
normal_hidden_rep = hidden_representation.predict(X_normal_scaled)
fraud_hidden_rep = hidden_representation.predict(X_fraud_scaled)
  
# Combining the encoded points into a single table 
encoded_X = np.append(normal_hidden_rep, fraud_hidden_rep, axis = 0)
y_normal = np.zeros(normal_hidden_rep.shape[0])
y_fraud = np.ones(fraud_hidden_rep.shape[0])
encoded_y = np.append(y_normal, y_fraud)
from sklearn.manifold import TSNE
# Plotting the encoded points
 
#Step 11: Splitting the original and encoded data into training and testing data

# Splitting the encoded data for linear classification
X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(encoded_X, encoded_y, test_size = 0.2)
  
# Splitting the original data for non-linear classification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
#Step 12: Building the Logistic Regression model and evaluating itâ€™s performance
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler 
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
from keras.layers import Input, Dense
from keras.models import Model, Sequential
from keras import regularizers

 
  
 

# Building the SVM model

svmclf = SVC()
start2 = time.time()
svmclf.fit(X_train, y_train)
stop2 = time.time()
#print(f"Classification time: {stop2 - start2}s")
# Storing the predictions of the non-linear model
y_pred_svmclf = svmclf.predict(X_test)
  
# Evaluating the performance of the non-linear model
print('Accuracy SVM: '+str(accuracy_score(y_test, y_pred_svmclf)))



from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_svmclf))


 
 
