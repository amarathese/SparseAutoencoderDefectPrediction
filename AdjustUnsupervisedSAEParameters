from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from keras.models import Model
from keras.layers import Dense, BatchNormalization, Dropout, Input
from keras import regularizers
from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve, auc
from sklearn.utils import shuffle
import tensorflow as tf

# Set seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load and preprocess data
df = pd.read_csv('/content/drive/My Drive/Data/xerces-1.2.csv')
df[df.columns[-1]] = df[df.columns[-1]].apply(lambda x: 1 if x != 0 else 0)
target = df[df.columns[-1]]
selected_metrics = ['wmc', 'dit', 'dam', 'cam', 'cbm']
features = df[selected_metrics]

# Number of folds for cross-validation
n_folds = 5
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)

# Variables to store average metrics
accuracy_scores = []
f1_scores = []
pr_auc_scores = []

# Iterate over folds
for fold, (train_index, val_index) in enumerate(skf.split(features, target)):
    print(f"\nFold {fold + 1}/{n_folds}")

    # Split data into training and validation sets
    x_train_fold, x_val_fold = features.iloc[train_index], features.iloc[val_index]
    y_train_fold, y_val_fold = target.iloc[train_index], target.iloc[val_index]

    # Apply SMOTE to balance class distribution for training set
    smote = SMOTE(random_state=42)
    x_train_fold_resampled, y_train_fold_resampled = smote.fit_resample(x_train_fold, y_train_fold)

    # Apply Min-Max scaling to features for training and validation sets
    scaler = MinMaxScaler()
    x_train_fold_resampled_scaled = scaler.fit_transform(x_train_fold_resampled)
    x_val_fold_scaled = scaler.transform(x_val_fold)

    # Define autoencoder architecture with improved parameters
    input_layer = Input(shape=x_train_fold_resampled_scaled.shape[1:])
    encoded = Dense(128, activation='relu', activity_regularizer=regularizers.l1(1e-4))(input_layer)
    encoded = BatchNormalization()(encoded)
    encoded = Dropout(0.4)(encoded)
    encoded = Dense(64, activation='relu', activity_regularizer=regularizers.l1(1e-4))(encoded)
    encoded = BatchNormalization()(encoded)
    encoded = Dropout(0.4)(encoded)
    decoded = Dense(64, activation='relu')(encoded)
    decoded = BatchNormalization()(decoded)
    decoded = Dropout(0.4)(decoded)
    decoded = Dense(x_train_fold_resampled_scaled.shape[1], activation='sigmoid')(decoded)

    autoencoder = Model(input_layer, decoded)
    autoencoder.compile(optimizer="adam", loss="mse", metrics=['accuracy'])

    # Train autoencoder
    autoencoder.fit(
        x_train_fold_resampled_scaled,
        x_train_fold_resampled_scaled,
        batch_size=8,
        epochs=100,
        shuffle=True,
        validation_split=0.20,
        verbose=0
    )

    # Create encoder model
    encoder = Model(inputs=input_layer, outputs=encoded)

    # Encode training and validation data
    x_train_encoded = encoder.predict(x_train_fold_resampled_scaled)
    x_val_encoded = encoder.predict(x_val_fold_scaled)

    # Define binary classification model
    classification_input = Input(shape=(x_train_encoded.shape[1],))
    classifier = Dense(64, activation='relu')(classification_input)
    classifier = BatchNormalization()(classifier)
    classifier = Dropout(0.3)(classifier)
    classifier = Dense(32, activation='relu')(classifier)
    classifier = Dropout(0.3)(classifier)
    classification_output = Dense(1, activation='sigmoid')(classifier)

    classification_model = Model(inputs=classification_input, outputs=classification_output)
    classification_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Train classifier
    classification_model.fit(
        x_train_encoded,
        y_train_fold_resampled,
        batch_size=8,
        epochs=100,
        shuffle=True,
        verbose=0
    )

    # Evaluate on validation set
    y_val_pred_prob = classification_model.predict(x_val_encoded).ravel()

    # Calculate metrics
    y_val_pred_binary = (y_val_pred_prob >= 0.5).astype(int)
    accuracy = accuracy_score(y_val_fold, y_val_pred_binary)
    f1 = f1_score(y_val_fold, y_val_pred_binary)
    precision, recall, _ = precision_recall_curve(y_val_fold, y_val_pred_prob)
    pr_auc = auc(recall, precision)

    # Store metrics
    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    pr_auc_scores.append(pr_auc)

    print(f"Fold {fold + 1} - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, PR-AUC: {pr_auc:.4f}")

# Average metrics across all folds
avg_accuracy = np.mean(accuracy_scores)
avg_f1 = np.mean(f1_scores)
avg_pr_auc = np.mean(pr_auc_scores)

print("\nAverage Metrics Across All Folds:")
print(f"Accuracy: {avg_accuracy:.4f}")
print(f"F1-Score: {avg_f1:.4f}")
print(f"PR-AUC: {avg_pr_auc:.4f}")
